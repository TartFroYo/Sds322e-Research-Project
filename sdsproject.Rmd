---
title: "Finding the Relationship between Federal Revenue for Public Schools and Violent Crimes, as well as Median Household Income and Violent Crimes"
author: "Aileen Li, Nyah Strickland"
date: "2023-03-04"
output: 
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

  For many years, a number of U.S. states have made changes to public school funding. One of our research questions is if there is a relationship between change in federal revenue for public schools and violent crimes by state during 1992-2016. We wanted to explore this because if there is a relationship, depending on what kind, states can make changes in their budget for the betterment of their enrolled students. We expected the more funding for public schools, the less violent crimes there are by state.
  
  As inflation increases annually, household incomes must also rise to adjust for standard of living in each state. Our second research question is if there is a relationship between median household income and violent crimes by state during 1992-2016. We wanted to explore this because if there is a relationship between these two datasets, then state governments should make changes in their policies to help states where households are most affected. We expect the greater the median household income, the less violent crimes there are by state. 

The following are the research questions we investigated:\
-__Is there a relationship between Federal Revenue for Public Schools and Crimes?__\
-__Is there a relationship between Median Household Income and Crimes?__\

And the following are the datasets we used:  
-[Median Household Income by State](https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h08.xls)\
-[U.S. Education Dataset](https://www.kaggle.com/datasets/noriuk/us-education-datasets-unification-project?resource=download)\
-[Crime Rate by State Dataset](https://corgis-edu.github.io/corgis/csv/state_crime/)\

The median household income, U.S. Education, and state crime rate datasets were acquired from the U.S. Census Bureau, U.S. Census Bureau and the National Center for Education Statistics (NCES), and Unified Crime Reporting Statistics and under the collaboration of the U.S. Department of Justice and the Federal Bureau of Investigation, respectively. We joined the data sets by the state names, which are the unique rows, and year. Specifically, we looked into the years from 1992 to 2016 for both research questions. For the median household income dataset, the unique variable is the median household income (Median income) in the current dollars dataset, measuring the annual state median household incomes without considering inflation. As for the U.S. Education dataset, the unique variable is federal revenue (FEDERAL_REVENUE), and for the violent crime rate dataset, the unique variables are violent crime rate (Data.Rates.Violent.All), nonviolent crime rate (Data.Rates.Property.All), burglary crime rate (Data.Rates.Property.Burglary), and murder rate (Data.Rates.Violent.Murder). The key variables are categorical while all the unique variables are numerical. These are the eight variables required for the project. The following are the links to the datasets: 

=======

``` {r}
library(tidyverse)
library(readr)
library(readxl)
```

```{r}
states_all_extended <- read_csv("states_all_extended.csv")
state_crime <- read_csv("state_crime.csv")
h08_1_ <- read_excel("h08 (1).xls")
```

## Is there a relationship between Federal Funding and Crimes?

## Is there a relationship between Median Household Income and Crimes?

# Tidying
For all the datasets, it is important to note we did not include Washington D.C as a state.

```{r}
# For all datasets, remove District of Columbia since it is not a state
# Only keep key and unique variables for each dataset 

# rename dataset to us_education
us_education <- states_all_extended %>% 
select('STATE', 'YEAR', 'FEDERAL_REVENUE') %>% 
filter(!str_detect(STATE, "^DISTRICT")) %>%
rename("State" = "STATE", "Year" = "YEAR",  "Federal_Revenue" = "FEDERAL_REVENUE") %>%
# Make sure state names are identical in both datasets
mutate(State = str_replace(State, "_", " "))

# Rename crime variable names so they don't have the word 'Data' in it
crime_rate <- state_crime %>% 
select('State', 'Year', 'Data.Rates.Violent.All', 'Data.Rates.Property.All', 'Data.Rates.Property.Burglary', 'Data.Rates.Violent.Murder') %>% 
filter(!str_detect(State, "^District")) %>%   rename("All_Property_Rates" = "Data.Rates.Property.All", "All_Property_Burglary_Rates" = "Data.Rates.Property.Burglary", "All_Violent_Rates" = "Data.Rates.Violent.All", "All_Murder_Rates" = "Data.Rates.Violent.Murder") %>% 
#put year in ascending order so it is easy to merge this    dataset with us_education 
arrange(Year)

# Make state names be uppercase to make merging the datasets easier
crime_rate$State <- toupper(crime_rate$State)
```

The median_housing_income dataset is untidy and contains unnecessary info in relation to what we wanted to analyze, so we modified it for our questions.  
We changed the column names in the dataset to be named in sequential order. Then, we deleted all the columns containing standard error data.

```{r tidy}
# find standard error columns and delete them
med_income_base = h08_1_
colnames(med_income_base) = paste(seq_along(med_income_base))
del_col = list()

for (col in 1:ncol(h08_1_)) {
  if (col!=1 & (col-1)%%2==0) {
    del_col= append(del_col, as.character(col))
  }
}
med_income_base = med_income_base[,!names(med_income_base) %in% del_col]
```

We renamed the columns to their respective titles given in the dataset on the fourth row. And then deleted rows containing unimportant information. 
There were two columns containing different info for the year 2013. We tried to find why they were different, in addition as to what the values in the parentheses meant, but there was no documentation for this dataset. Thus, we decided to keep the first column instance of the year 2013 so that we would have no differing data for the same year and state combination that could possibly affect our graphs.

```{r tidy}
# rename columns
colnames(med_income_base) = med_income_base[4,]

# remove undesired data
med_income_base = med_income_base[-(1:6),-9]

i = min(which(med_income_base$State=='Wyoming'))
med_income_untidy = med_income_base[1:i,] |> filter(State!="D.C.")
```

We made the median_housing_income dataset tidy by changing the individual year columns to be under the Year variable and their values under column Median_Income. Then, we filtered for observations within our desired time period, which is from 1992 to 2016 and arranged them in ascending order.

```{r tidy}
# make desired data tidy
med_income_tidy = med_income_untidy |> 
  pivot_longer(!State, names_to="Year", values_to="Median_Income") |> 
  filter(substr(Year, 1, 4) %in% (1992:2016)) |> arrange(Year)
med_income_tidy$State = toupper(med_income_tidy$State)
```

We found years with values in parentheses and checked for duplicate years for the same state.
We also removed the values in parentheses for the rest of the years. And for the sake of merging the datasets easier, we completely capitalized the state names.

```{r tidy}
# find years with () and duplicate years other than 2013
pyears = med_income_tidy[grep(")$",med_income_tidy$Year),]
dupl_years = pyears |>
  mutate(compare=substr(Year,1,6)) |>
  select(State, compare) |> group_by(State, compare) |>
  filter(duplicated(compare))

# remove () from some years
med_income_tidy$Year = substr(med_income_tidy$Year, 1,4)
med_income = med_income_tidy
```

# Joining/Merging
```{r}
# Note how many observations are in each data set
nrow(us_education)
ncol(us_education)
nrow(crime_rate)
ncol(crime_rate)
# Left join state_crime to us_education by key variable Year and State
# Left joining crime_rate to us_education makes the question1 dataset start with 1992 
# Filter missing values of federal revenue 
research_questions <- left_join(us_education,crime_rate, by = c("Year", "State")) %>% filter(!is.na(Federal_Revenue)) 
research_questions
nrow(research_questions)
```

There are 1682 observations and 1 unique variable (Federal_Revenue) in the us_education dataset while in the state_crime dataset there are 3055 observations and 4 unique variables (All_Property_Rates, All_Property_Burglary_Rates, All_Violent_Rates, All_Murder_Rates). Between us_education and state_crime, there are 2 ID variables (Year and State) in common. After joining these 2 datasets into a single dataset research_questions, there are 1682 observations since the us_education spans from 1992-2016, causing the merged dataset to stop at that time frame. This is also the time period we wanted. There are 1250 observations in the question1 dataset. This means that from the total number of overlapped observations (being 3055-1682=1373) in both us_education and state_crime datasets, there were 1373-1250=123 observations that had missing values. Since the crime_rate was left joined to us_education, the missing values likely arose from not matching with the year and state name for Federal_Revenue as this variable records state public school federal funding from 1992-2016.

# Wrangling
```{r}

```

# Visualizing
```{r}

```

# Discussion
